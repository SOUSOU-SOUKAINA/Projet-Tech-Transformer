# -*- coding: utf-8 -*-
"""Transformer2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nAT8bup0ucPuO4e2RObY6ibhLFVa67dN

# **Classes Embedding et Positional Encoding**

Avant d'envoyer les données dans le modèle Transformer, nous devons représenter les mots sous une forme exploitable par le réseau de neurones.
Pour cela, on utilise deux étapes clés :



*   Encodage des mots en vecteurs (InputEmbedding).
*   Ajout d'une information de position (PositionalEncoding).

 **Objectif de la classe InputEmbedding : Transformer chaque mot en un vecteur
de dimension d_model**
**Objectif de la classe PositionalEncoding: Ajouter une information de position aux embeddings**
"""

import torch
import torch.nn as nn
import math
class InputEmbedding(nn.Module):
  def __init__(self, d_model:int ,vocab_size:int): # on doit donner le dimension de model et le nombre de mots dans le vocabulaire
    super().__init__()
    self.d_model = d_model
    self.vocab_size = vocab_size
    # pytorch already provide Embedding layer en utilisant le mapping (clé:valeur)
    self.embedding = nn.Embedding(vocab_size, d_model) # nn.Embedding () crée une couche ou chaque mot dans le vocab est représenté par un vecteur de dimension d_model
  def forward(self, x):
    return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
  def __init__(self, d_model:int,seq_len:int, dropout:float) -> None:
    super().__init__()
    self.d_model = d_model
    self.seq_len = seq_len
    self.dropout = nn.Dropout(dropout)

    #create a matrix of shape (seq-len,d_model)
    pe = torch.zeros(seq_len, d_model)
    #create a vector of shape (seq_len) (on veut créer positionnal vector pour cela on applique les formules)
    position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) # il s'agit d'un tensor et vecteur de dimension(seq_len,1)
    div_term=torch.exp(torch.arange(0,d_model,2)).float() * (-math.log(10000.0)/d_model)
    #apply the sin to even positions
    pe[:,0::2]=torch.sin(position * div_term)
    #apply the cos to odd positions
    pe[:,1::2]=torch.cos(position * div_term)
    #Dans le Transformer, on travaille avec des batches de phrases, donc nos tenseurs doivent être en 3D ,
    #Mais comme l'encodage positionnel est le même pour toutes les phrases du batch, on ne veut pas en créer un différent pour chaque phrase.
    #On utilise unsqueeze(0) pour avoir une forme [1, seq_len, d_model], ce qui nous permet de l'ajouter facilement à un batch entier
    pe=pe.unsqueeze(0)
    #Si on écrivait self.pe = nn.Parameter(pe), PyTorch le considérerait comme un paramètre entraînable, ce que nous ne voulons pas car les encodages positionnels sont fixes.
    #Avec self.register_buffer('pe', pe), PyTorch stocke pe dans le modèle sans qu'il soit mis à jour pendant l'entraînement.
    self.register_buffer('pe',pe)
  def forward(self,x):
    x=x+(self.pe[:, :x.shape[1], :]).requires_grad_(False) # pour ne pas entrainer pe car il est fixe ne change pas
    #now we apply the dropout
    return self.dropout(x)

"""# **Class: LayerNormalization**
La Layer Normalization est une technique essentielle dans les Transformers pour stabiliser l'entraînement et améliorer la convergence du modèle.
"""

import torch
import torch.nn as nn
import math

class LayerNormalization(nn.Module):
    def __init__(self, eps=10**-6) -> None:
        super().__init__()
        self.eps = eps  # Petite valeur ajoutée pour éviter la division par zéro lors de la normalisation.

        # Paramètre d'échelle (gamma) : Facteur multiplicatif, initialisé à 1, et appris pendant l'entraînement.
        self.alpha = nn.Parameter(torch.ones(1))

        # Paramètre de biais (beta) : Décalage additif, initialisé à 0, et appris pendant l'entraînement.
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        """
        Applique la normalisation de couche à l'entrée x.
        Chaque vecteur dans la dernière dimension (d_model) est normalisé indépendamment.
        """
        # Calcul de la moyenne sur la dernière dimension (d_model) de chaque élément du batch.
        # keepdim=True permet de garder la même forme que x (évite une réduction de dimension).
        mean = x.mean(dim=-1, keepdim=True)

        # Calcul de l'écart-type sur la dernière dimension (d_model).
        std = x.std(dim=-1, keepdim=True)

        # Normalisation : (x - moyenne) / (écart-type + eps) pour éviter division par zéro.
        # Ensuite, on applique l'échelle alpha (gamma) et le décalage beta.
        return self.alpha * (x - mean) / (std + self.eps) + self.bias

"""# **Class: FeedForwardBlock**
Le Feed-Forward Block est un composant essentiel du Transformer, utilisé après chaque couche d’attention. Son rôle est de transformer chaque embedding indépendamment, en appliquant une suite de transformations linéaires et non linéaires.
"""

import torch
import torch.nn as nn

class FeedForwardBlock(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:
        """
        Implémente un bloc Feed-Forward utilisé dans les Transformers.

        Paramètres :
        - d_model : Taille de l'embedding d'entrée et de sortie (dimension du modèle).
        - d_ff : Taille de la couche cachée dans le réseau feed-forward (souvent plus grande que d_model).
        - dropout : Taux de dropout pour éviter le sur-apprentissage.
        """
        super().__init__()

        # Première couche linéaire : Projette de d_model à d_ff (augmentation de dimension).
        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 et B1

        # Couches de dropout pour régularisation (évite le sur-apprentissage).
        self.dropout = nn.Dropout(dropout)

        # Deuxième couche linéaire : Réduit la dimension de d_ff à d_model (rétablit la taille d'origine).
        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 et B2

    def forward(self, x):
        """
        Passe avant du bloc Feed-Forward.

        Entrée :
        - x : Tenseur de forme (batch_size, seq_len, d_model).

        Étapes :
        1. Applique la transformation linéaire `Linear_1` : (d_model → d_ff).
        2. Applique la fonction d'activation ReLU (introduit de la non-linéarité).
        3. Applique le Dropout (désactive aléatoirement des neurones pour éviter le sur-apprentissage).
        4. Applique la transformation linéaire `Linear_2` : (d_ff → d_model) pour revenir à la dimension initiale.

        Sortie :
        - Tenseur de même forme que l'entrée (batch_size, seq_len, d_model).
        """

        # x (batch_size, seq_len, d_model) --> Linear_1 (d_model → d_ff)
        # --> ReLU --> Dropout --> Linear_2 (d_ff → d_model) --> y (batch_size, seq_len, d_model)
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))

"""# **Class: MultiHeadAttentionBlock**
Le Multi-Head Attention Block est l’un des éléments clés du Transformer. Il permet au modèle de capturer différentes relations contextuelles entre les mots d’une phrase en divisant l’espace d’attention en plusieurs "têtes" indépendantes.
"""

import torch
import torch.nn as nn
import math

class MultiHeadAttentionBlock(nn.Module):
    def __init__(self, d_model:int,h:int,dropout:float) ->None:
        super().__init__()
        self.d_model = d_model
        self.h = h
        assert d_model % h == 0, "d_model must be divisible by h" # assert est utilisée pour vérifier qu’une condition est vraie. Si la condition est fausse, une exception AssertionError est levée
        self.d_k = d_model // h
        self.d_v = d_model // h
        self.W_q = nn.Linear(d_model,d_model) #Wq
        self.W_k = nn.Linear(d_model,d_model) #Wk
        self.W_v = nn.Linear(d_model,d_model) #Wv

        self.W_o = nn.Linear(d_model,d_model) #Wo
        self.dropout = nn.Dropout(dropout)

    @staticmethod   #pour ne pas créer d'instance de classe quand on appelle la méthode
    def attention(query,key,value,mask,dropout:nn.Dropout):

        d_k = query.shape[-1]  # the last dimensionnal between the key and the value

        #(Batch,h,seq_len,d_k)  ---> (Batch,h,seq_len,seq_len) cette passage grace à la multiplication de Q avec K-transpose
        attention_scores = torch.matmul(query,key.transpose(-2,-1)) / math.sqrt(d_k)
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask==0,-1e9)   # si mask est vrai  (Remplace les positions où le masque vaut 0 par une valeur très négative ) et a le meme dimension que la matrice attention scores (Batch,h,seq_len,seq_len) sauf que h=1 pour appliquer le meme masque à chaque tete
        attention_scores = attention_scores.softmax(dim=-1)  #(Batch,h,seq_len,seq_len) donc la derniere dimenesion c'est seq_len
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        return torch.matmul(attention_scores,value),attention_scores

    def forward(self,q,k,v,mask): # le mask if we want some words to not interact with other words we mask them

        query = self.W_q(q) #(Batch,seq_len,d_model) --> (Batch,seq_len,d_model)
        key = self.W_k(k)  #(Batch,seq_len,d_model) --> (Batch,seq_len,d_model)
        value = self.W_v(v) #(Batch,seq_len,d_model) --> (Batch,seq_len,d_model)

        query=query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2) #(Batch,seq_len,d_model) --> (Batch,seq_len,h,d_k) -->(Batch,h,seq_len,d_k)
        key=key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)
        value=value.view(value.shape[0],value.shape[1],self.h,self.d_v).transpose(1,2)

        x,self.attention_scores=MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)

        #(Batch,h,seq_len,d_k) --> (Batch,seq_len,h,d_k) --> (Batch,seq_len,d_model)
        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)
        return self.W_o(x) # (Batch,seq_len,d_model)

"""# **Class: ResidualConnection**
*c'est la partie ou on utilise par exemple une copie de Query (input embedding) avec la sortie de la couche précédente (ici par exemple multi-head attention) et les combiner dans add & norm,,,,
on skipe une couche (skip connection between add & norm and the previos layer *
"""

import torch
import torch.nn as nn
import math

class ResidualConnection(nn.Module):
    def __init__(self,dropout:float) ->None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.norm = LayerNormalization()
    def forward(self,x,sublayer):
        return x + self.dropout(sublayer(self.norm(x))) #the definition of add & norm

"""# **Class: EncoderBlock**
On regroupe ce qu'on a fait avant
"""

import torch
import torch.nn as nn
import math

class EncoderBlock(nn.Module):
    def __init__(self,self_attention_block:MultiHeadAttentionBlock,feed_forward_block:FeedForwardBlock,dropout:float) ->None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # une liste contenant deux residual connection

    def forward(self,x,src_mask):  # the mask that we want to apply to the input decoder
        x=self.residual_connections[0](x,lambda x:self.self_attention_block(x,x,x,src_mask)) # le premier block qui contient le multi-head attention et add & norm
        x=self.residual_connections[1](x,self.feed_forward_block) # le deuxieme block de feedforward et add&norm
        return x

"""# **Class: Encoder**"""

# on a N encoder comme mentionné dans le paper

class Encoder(nn.Module):
    def __init__(self,layers:nn.ModuleList) ->None:
        super().__init__()
        self.layers=layers
        self.norm=LayerNormalization()
    def forward(self,x,mask):
        for layer in self.layers:
            x=layer(x,mask)
        return self.norm(x)

"""# **Class: DecoderBlock**"""

import torch
import torch.nn as nn
import math

class DecoderBlock:
    def __init__(self,self_attention_block: MultiHeadAttentionBlock,cross_attention_block: MultiHeadAttentionBlock,feed_forward_block: FeedForwardBlock,dropout: float):
        self.self_attention_block=self_attention_block
        self.cross_attention_block=cross_attention_block
        self.feed_forward_block=feed_forward_block
        self.dropout=nn.Dropout(dropout)
        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])
    def forward(self,x,encoder_output,src_mask,tgt_mask): # src mask come from encoder and tgt mask from decoder (we have translation example that's why there are 2 masks)
        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,tgt_mask))# x x x means query key value , they are the same
        x=self.residual_connections[1](x,lambda x: self.cross_attention_block(x,encoder_output,encoder_output,src_mask))
        x=self.residual_connections[2](x,self.feed_forward_block)
        return x

"""# **Class: Decoder**"""

class Decoder(nn.Module):
    def __init__(self,layers:nn.ModuleList) -> None:
        super().__init__()
        self.layers=layers
        self.norm=LayerNormalization()
    def forward(self,x,encoder_output,src_mask,tgt_mask):
        for layer in self.layers:
            x=layer(x,encoder_output,src_mask,tgt_mask)
        return self.norm(x)

"""# **Class: ProjectionLayer**"""

class ProjectionLayer(nn.Module):
    def __init__(self,d_model:int,vocab_size:int) -> None:
        super().__init__()
        self.proj=nn.Linear(d_model,vocab_size)
    def forward(self,x):
      #(Batch,seq_len,d_model) --> (Batch,seq-len,vocab_size)
      return torch.log_softmax(self.proj(x),dim= -1)

"""# **Class: Transformer**"""

class Transformer(nn.Module):
  def __init__(self,encoder:Encoder,decoder:Decoder,src_embed:InputEmbeddings,tgt_embed:InputEmbeddings,src_pos:PositionalEncoding,tgt_pos:PositionalEncoding,projection_layer:ProjectionLayer) -> None:
    super().__init__()
    self.encoder=encoder
    self.decoder=decoder
    self.src_embed=src_embed
    self.tgt_embed=tgt_embed
    self.src_pos=src_pos
    self.tgt_pos=tgt_pos
    self.projection_layer=projection_layer
  def encode(self,src,src_mask):
    src=self.src_embed(src)
    src=self.src_pos(src)
    return self.encoder(src,src_mask)
  def decode(self,encoder_output,src_mask,tgt,tgt_mask):
    tgt=self.tgt_embed(tgt)
    tgt=self.tgt_pos(tgt)
    return self.decoder(tgt,encoder_output,src_mask,tgt_mask)
  def project(self,x):
    return self.projection_layer(x)
 #on a pas ecnore lie tout ça et donner les hyperparametres
 def builf_transformer(src_vocab_size:int,tgt_vocab_size:int,src_seq_len:int,tgt_seq_len:int,d_model:int=512,N:int=6,h:int=8,dropout=0.1,d_ff:int=2048) -> Transformer: # N is the number of layer (number of encoder or decoder blocks ),, d_ff : header layer in feedforward
  # create the embedding layers
    src_embed=InputEmbeddings(d_model,src_vocab_size)
    tgt_embed=InputEmbeddings(d_model,tgt_vocab_size)
  #create the positionnal encoding layers
    src_pos=PositionalEncoding(d_model,src_seq_len,dropout)
    tgt_pos=PositionalEncoding(d_model,tgt_seq_len,dropout)
  #create the encoder blocks
    encoder_blocks=[]
    for _ in range(N):
      encoder_self_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)
      feed_forward_block=FeedForwardBlock(d_model,d_ff,dropout)
      encoder_block=EncoderBlock(encoder_self_attention_block,feed_forward_block,dropout)
      encoder_blocks.append(encoder_block)
  # create the decoder blocks
    decoder_blocks=[]
    for _ in range(N):
      decoder_self_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)
      decoder_cross_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)
      feed_forward_block=FeedForwardBlock(d_model,d_ff,dropout)
      decoder_block=DecoderBlock(decoder_self_attention_block,decoder_cross_attention_block,feed_forward_block,dropout)
      decoder_blocks.append(decoder_block)
  #create the encoder and the decoder
    encoder=Encoder(nn.ModuleList(encoder_blocks))  # we give him all its blocks : N
    decoder=Decoder(nn.ModuleList(decoder_blocks))  # the same
  #create the projection layer
    projection_layer=ProjectionLayer(d_model,tgt_vocab_size)
  # create the transformer
  transformer=Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer)
  # initialise the parameters using Xavier
  for p in transformer.parameters():
    if p.dim()> 1:
      nn.init.xavier_uniform_(p)
  return transformer